#### Layer normalization
[你是怎样看待刚刚出炉的 Layer Normalisation 的？](https://www.zhihu.com/question/48820040/answer/113348156)
[LN 一个好的解释](https://zhuanlan.zhihu.com/p/60821628)
[深度学习中的Normalization](https://zhuanlan.zhihu.com/p/43200897)


## 1 参数初始化

[神经网络权重矩阵初始化的意义？](https://www.zhihu.com/question/291032522/answer/605843215)
[常见初始化方法](https://zhuanlan.zhihu.com/p/72249910)
[深度学习参数初始化（weights initializer）策略大全](https://blog.csdn.net/qq_27825451/article/details/88707423)
[深度学习中的参数初始化](https://blog.csdn.net/mzpmzk/article/details/79839047)
[关于参数初始化的若干问题以及Xavier、He初始化推导](https://zhuanlan.zhihu.com/p/40175178)
[可视化超参数作用机制：二、权重初始化](https://zhuanlan.zhihu.com/p/38315135)
[Hyper-parameters in Action! Part II — Weight Initializers](https://towardsdatascience.com/hyper-parameters-in-action-part-ii-weight-initializers-35aee1a28404)
[PyTorch 学习笔记（四）：权值初始化的十种方法](https://blog.csdn.net/u011995719/article/details/85107122#Xavierkaiming_7)

问题1：初始化不能初始化为0或者相等
防止神经元的值相等导致最后权重统一更新，防止神经元的无区分性
问题2：初始化不能初始化太大
防止sigmoid，tanh饱和，造成梯度太小从而梯度弥散
问题3：初始化不能太小
防止最后输出值太小，造成后向传播时梯度太小而梯度弥散

初始化必要条件一：各层激活值不会出现饱和现象。
初始化必要条件二：各层激活值不为0。

高斯分布初始化(方差是与每层的神经元个数相关): 增加神经元的差异性,但是会造成方差的逐层递减,训练难度会增大
Xavier: 核心：方差不变性,但是没有考虑到激活函数(基于线性函数且对称,如果是在小区间内对tanh可以使用)
HeKaiming: 核心: 仍然是方差不变性,但是考虑到了激活函数对数据分布的影响

### 1.1 Xavier 初始化方法
对tanh激活函数有很强的作用
对relu和sigmoid没多大的作用
主要使用来计算方差的
xavier_uniform
xavier_normal
核心思想：数据在层与层传递时候(无论是前向还是后向传播)方差需要保持不变性.
如果不这样会怎样：假如越来越大则会导致后面的数据越来越扩张前向时导致sigmoid,tanh在饱和区,后向时有梯度消失的风险;另外还有数据溢出的风险.假如数据越来越小会怎样？则会导致后面的数据差异越来越小,不易产生有效的梯度
所以依据此思想分别从前向和后向来考虑权重的方差应该是怎样的形式才会保证数据的方差在前向和后向传递时方差的不变性.
[推导过程-深度学习中Xavier初始化](https://www.cnblogs.com/hejunlin1992/p/8723816.html)

### 1.2 He (kaiming) 初始化方法
同样是基于Xavier的思想,但是激活函数是Relu和leakyRelu
考虑了激活函数对数据分布的影响

### 1.3 orthogonal initialize(正交初始化方法)
[RNN的梯度消失/爆炸与正交初始化](https://blog.csdn.net/shenxiaolu1984/article/details/71508892)
[Explaining and illustrating orthogonal initialization for recurrent neural networks](https://smerity.com/articles/2016/orthogonal_init.html)
主要用来解决最开始时RNN梯度爆炸和梯度消失问题
初始化为单位正交阵(正交阵的特征值为1)
使用正交矩阵初始化，对于矩阵（如果是张量，按最后一维reshape成矩阵），使用随机初始化的正态分布矩阵进行QR，使用Q矩阵来初始化，  Q为正交阵
[QR分解](https://wenku.baidu.com/view/5768eb134431b90d6c85c770.html)
