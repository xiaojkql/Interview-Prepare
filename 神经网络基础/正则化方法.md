- l1/l2正则化
- 权重衰减
- 提前停止
- 丢弃法(dropout)
- 数据增强(data augmentation)

## 1 L1/L2正则化方法

[正则化方法：L1和L2 regularization、数据集扩增、dropout](https://blog.csdn.net/u012162613/article/details/44261657)

[神经网络正则化(1)：L1/L2正则化](https://zhuanlan.zhihu.com/p/35893078)
[神经网络正则化(2)：dropout正则化](https://zhuanlan.zhihu.com/p/35948928)

## 2 权重衰减
[权重衰减（weight decay）与学习率衰减（learning rate decay）](https://zhuanlan.zhihu.com/p/38709373)
[L2正则=Weight Decay？并不是这样](https://zhuanlan.zhihu.com/p/40814046)

## 3 提前停止


## 4 丢弃法(dropout)

[Dropout解决过拟合问题](https://zhuanlan.zhihu.com/p/23178423)


## 5 数据增强
