为什么使用batch-normalization
1、提高梯度在网络中的流动。Normalization能够使特征全部缩放到[0,1]，这样在反向传播时候的梯度都是在1左右，避免了梯度消失现象。
2、提升学习速率。归一化后的数据能够快速的达到收敛。
3、减少模型训练对初始化的依赖。

[详解深度学习中的Normalization，BN/LN/WN](https://zhuanlan.zhihu.com/p/33173246)

[batch normalization](https://zhuanlan.zhihu.com/p/34879333)
[深度学习中 Batch Normalization为什么效果好？](https://www.zhihu.com/question/38102762/answer/85238569)

# Batch Normalization
要解决的问题: 类内协变量转移(internal covatiate shift)
在训练过程中,权重矩阵和偏移偏量都在不断的更新,而这样会导致一层输出的分布相对于一层的输入是变化的,最终导致w,b会不断的去适应不断改变的分布的数据.
问题:
- 分布不断变化: 参数更新不断适应,就会导致学习速率缓慢(解决: 让分布呈现一个稳定的形式)
- 饱和激活函数,梯度饱和区 (两种解决方式: 替换为非饱和激活函数, 让分布呈现一个稳定的分布形式)

原始解决方式: 每一层的输入出加一层白化,但是这种方式计算量很大,不适合
BN具有的特性: 速度快, 有类似白化的作用, 能够保留前层网络学习到的东西

每个特征: 归一化为均值为0，方差为1，然后加一个变化，使得变化后的分布类似于原来的数据

好处:
- 数据分布相对稳定 --> 层与层的解耦
- 对网络参数的变化不敏感 (原来随着层数的增加会放大参数变化), 容易设置参数, 更容易调参
- 能使用饱和激活函数
- 具有一定的正则化作用: 每个batch的均值和方差不一致,就相当于引进了随机误差
