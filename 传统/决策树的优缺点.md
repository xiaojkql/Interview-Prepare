可解释性强
• 可处理混合类型特征 (数据类型和属性类型可同时进行处理)
• 具体伸缩不变性（不用归一化特征, 特征之间值的差别影响不是很大）
• 有特征组合的作用
• 可自然地处理缺失值
• 对异常点鲁棒
• 有特征选择作用
• 可扩展性强， 容易并行


• 缺乏平滑性（回归预测时输出值只能
输出有限的若干种数值）
• 不适合处理高维稀疏数据

决策树的优点
相对于其他数据挖掘算法，决策树在以下几个方面拥有优势：
- 决策树易于理解和实现. 人们在通过解释后都有能力去理解决策树所表达的意义。
- 对于决策树，数据的准备往往是简单或者是不必要的 . 其他的技术往往要求先把数据一般化，比如去掉多余的或者空白的属性。
- 能够同时处理数据型和常规型属性。其他的技术往往要求数据属性的单一。
- 在相对短的时间内能够对大型数据源做出可行且效果良好的结果。
- 对缺失值不敏感
- 可以处理不相关特征数据
- 效率高，决策树只需要一次构建，反复使用，每一次预测的最大计算次数不超过决策树的深度。

决策树的缺点
1)对连续性的字段比较难预测。

2)对有时间顺序的数据，需要很多预处理的工作。

3)当类别太多时，错误可能就会增加的比较快。

4)一般的算法分类的时候，只是根据一个字段来分类。

5)在处理特征关联性比较强的数据时表现得不是太好


优点：天然的可解释性。  这是决策树最大的优点了。  可解释性有两方面的考虑。 一方面， 树结构的理解不需要机器学习专家来解读。 另一方面， 很容易转化成规则。可以处理缺失值（missing）， 字符型（nominal）， 数值型（numeric）等数据类型。非参数模型（non-parametric）。 没有复杂的参数设置，谁跑效果都相对一样。对相关（Correlation）属性能够比较好的处理。运算速度相对比较快。缺点：最大的缺点就是很容易过拟合。 导致实际预测的效果并不高。决策树中，重复的部分过多难以概括， 譬如对于 ( F1 && F2 ) || ( F3 && F4 ) 的表达（如下图，划圈的部分重复）， 决策树就有很大的重复。不适合处理高维数据， 当属性数量过大的时候， 部分决策树就不太适用了。对异常值（Outlier）过于敏感， 很容易导致树的结构的巨大的变换。泛化（Generalization）能力太差， 对于没有出现过的值几乎没有办法。


[怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢?](https://www.zhihu.com/question/58230411)
