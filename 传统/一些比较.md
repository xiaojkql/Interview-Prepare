[决策树,GBDT,XGBoost](https://zhuanlan.zhihu.com/p/34534004)
[机器学习算法中 GBDT 和 XGBOOST 的区别有哪些？](https://www.zhihu.com/question/41354392)
- GBDT 以CART作为基学习器， xgboost可以自己设置基学习器
- XGBoost用到了二阶信息且可以自定义损失函数，GBDT只用到了一阶导数信息
- XGBoost用了正则项(两个叶节点数量-->转换为了特征分裂的阀值,相当于作了预剪枝处理,叶节点上的值的L2正则,相当于作了平滑处理)，正则降低了variance
- shringkage (eta,实际使用时，将eta设置的小，itration设置的大一点，可以防止过拟合)
- XGBoost用到了列抽样，降低过拟合，降低计算量，相当于RandomForest
- 对缺失值的处理，自动学习它的分分类方向
- 支持并行 实在特征粒度上的并行 将数据事先进行预排序和block,后面每一次计算时都用block计算在特征上进行并行化处理，使得在寻找最佳分裂点的时候能够并行化计算
- XGBoost相对于LightGBM内存开销大 要预排序
- XGBoost先建树到满树，然后进行剪枝，而GBDT遇到不能建树就停止，可能错过了一些较好的分裂点
- 实现了一种分裂节点寻找的近似算法,即分位的原理,近似直方图原理
- 节点分裂算法能自动利用特征的稀疏性
- 直接用XGBoost打分函数（包括了损失与正则两部分）去指导树的生成，避免了line search。而GBDT则是用平方（对数）损失指导树的生成，再做line search
- XGBoost打分函数综合了loss和正则，避免了决策树容易过拟合的问题
- 传统的GBDT每轮使用全部数据，XGBoost支持对数据进行采样。


[LightGBM大战XGBoost，谁将夺得桂冠？](https://mp.weixin.qq.com/s/JQasgzl-EpqBey7W6jKCTw)
