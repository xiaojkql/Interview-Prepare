没有免费的午餐定理
没有免费的午餐，不存在一个算法能解决所有问题，特定问题适用于不同的算法，所以不能一蹴而就，必须特定问题特殊分析

最小描述长度定理

occma剃须刀定理

偏差和方差

泛化能力

[关于树的几个ensemble模型的比较（GBDT、xgBoost、lightGBM、RF）](https://blog.csdn.net/xwd18280820053/article/details/68927422)
[大比较](https://www.cnblogs.com/infaraway/p/7890558.html)

### Bagging
- 降低了方差，不改变偏差
- 适合于对训练数据集很敏感的学习器
- 缺点：当基学习器相关时效果不明显

#### 随机森林
[博客](https://zhuanlan.zhihu.com/p/22097796)
一种特殊的bagging
在学习树的时候还要进行一次boostrap选择特征集
- 降低了学习树之间的相关性，更进一步降低了方差


### Boosting
- 降低偏差，增加方差(模型函数空间增大)，最后会增加集成模型的复杂度(函数空间变得很大)，从而增大了方差，增加了总体的损失
- Boosting是一种加法模型(additive training)
- NP难问题，通过贪心法求解局部最优解

[GBDT、XGBoost、LightGBM 的使用及参数调优](https://zhuanlan.zhihu.com/p/33700459)

#### Adaboost
- (1) 初始样本权重分布来拟合分类器
- (2) 根据上一个分类器的分类结果，重新计算样本的权重分布和上一个分类器的分类权重
- (3) 根据新的样本权重分布拟合新的分类器，返回到(2)，直至计算完要求的所有基分类器的个数
- (4) 根据每个分类器的权重进行linear combination

#### GBDT (Gradient boosting decision tree)
- 初始函数
- 求关于函数的梯度
- 用基模型拟合梯度
- 在原函数上进行梯度下降得到新函数

#### XGBoost (Newton boosting decision tree)
- 正则项-对每颗树的复杂度进行惩罚：相对于GBDT不容易过拟合
- 树的正则化：树的深度，树的叶节点数量，内部节点个数，叶节点分数
- XGBoost的正则化项有：叶节点数量，和叶节点分数
- 增益：ID3-信息增益，C4.5-信息增益比，CART-Gini系数
- XGBoost: 利用了二阶泰勒公式的相关结果，定义了新的打分函数 (选取特征)
- 树节点分裂方法：对选取的节点的值进行分裂方法：近似方法--根据特征的样本个数选取某一些分位数作为分裂点的值
- 稀缺值的处理
- 显式地将树模型的复杂度作为正则项加在优化目标
##### XGBoost 正则化项
- 怎样使用的正则项(巧妙的将正则项融入树的构造，从损失函数一步一步的推导出树构造时的评分函数-使用了二阶相对精确，以叶节点来计算新的评分函数)
- 正则化包含了叶节点的个数和每个叶节点的值的L2正则化项
##### XGboost 并行处理
[BLOG](http://zhanpengfang.github.io/418home.html)
- 对每一个level 的 node splitting 进行并行化同时处理：由于每个节点包含的样本数量是不一致的，所以每个节点的处理速度是不一样的
- 在处理feature(选择)时进行并行化，针对某个feature然后进行每个分裂点并行
- 提前将样本进行排序，产生feature spliting然后并行化每个feature spliting

##### XGBoost 自定义损失函数
允许用户定义自定义优化目标和评价标准

##### XGBoost 缺失值处理方法
[怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢?](https://www.zhihu.com/question/58230411)
[xgboost是如何处理缺失值的](https://www.jianshu.com/p/a9f49e08be86)
[决策树处理缺失值](https://blog.csdn.net/u012328159/article/details/79413610)

##### XGBoost 剪枝的方法
分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。
XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。
这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂

##### XGBoost内置交叉验证方法可以选择合适的参数
GBM不能，使用的Grid search 选择少量的参数

#### XGBoost参数调节
三类参数：通用参数，Booster参数，学习目标(训练函数)参数
基模型的复杂度：gama(最小的增益),max_depth,min_child_weight,
随机化抽样: subsample,col_sample,
正则化项：alpha,lambda
[参数调节详细介绍](https://blog.csdn.net/han_xiaoyang/article/details/52665396)
[xgboost参数介绍](https://blog.csdn.net/zc02051126/article/details/46711047)
[GBM参数调节](https://blog.csdn.net/han_xiaoyang/article/details/52663170)
[xgboost参数调节-知乎](https://zhuanlan.zhihu.com/p/52501965)
[xgboost-调参-知乎](https://zhuanlan.zhihu.com/p/35061092)
eta(缩减因子 避免过拟合),

#### lightGBM
- 直方图算法


### Stacking (模型融合方法)
[博客,比较详细解释了stacking](https://blog.csdn.net/maqunfi/article/details/82220115)
[stacking原理图](https://blog.csdn.net/zkx1949/article/details/83273914)

