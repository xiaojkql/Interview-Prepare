没有免费的午餐定理
没有免费的午餐，不存在一个算法能解决所有问题，特定问题适用于不同的算法，所以不能一蹴而就，必须特定问题特殊分析

最小描述长度定理

occma剃须刀定理

偏差和方差

泛化能力

[关于树的几个ensemble模型的比较（GBDT、xgBoost、lightGBM、RF）](https://blog.csdn.net/xwd18280820053/article/details/68927422)
[大比较](https://www.cnblogs.com/infaraway/p/7890558.html)

### Bagging
- 降低了方差，不改变偏差
- 适合于对训练数据集很敏感的学习器
- 缺点：当基学习器相关时效果不明显

#### 随机森林
[博客](https://zhuanlan.zhihu.com/p/22097796)
在特征列和数据的处理上作了文章
一种特殊的bagging
在学习树的时候还要进行一次boostrap选择特征集
- 降低了学习树之间的相关性，更进一步降低了方差
随机森林的用途: 分类,回归,无监督学习聚类,异常点检测
采样: 数据采样(有放回,行),特征采样(无放回?,列)
建树的时候无需进行剪枝,因为两个随机采样增加了训练样本的多样性,最终结果不会导致过拟合

优点:
- 不会过拟合
- 抗噪声能力强
- 处理输入特征的高维性(无需进行特征选择)

防止过拟合：
- 限制树的深度
- 限制分裂节点最小样本数
- 限制叶子节点最小样本数
- 限制节点分裂所带来的min_impurity_decrease，
- 限制节点分裂所需的最小不纯度



### Boosting
- 降低偏差，增加方差(模型函数空间增大)，最后会增加集成模型的复杂度(函数空间变得很大)，从而增大了方差，增加了总体的损失
- Boosting是一种加法模型(additive training)
- NP难问题，通过贪心法求解局部最优解

[GBDT、XGBoost、LightGBM 的使用及参数调优](https://zhuanlan.zhihu.com/p/33700459)

#### Adaboost
- (1) 初始样本权重分布来拟合分类器
- (2) 根据上一个分类器的分类结果，重新计算样本的权重分布和上一个分类器的分类权重
- (3) 根据新的样本权重分布拟合新的分类器，返回到(2)，直至计算完要求的所有基分类器的个数
- (4) 根据每个分类器的权重进行linear combination

#### GBDT (Gradient boosting decision tree)
- 初始函数
- 求关于函数的梯度
- 用基模型拟合梯度
- 在原函数上进行梯度下降得到新函数

#### XGBoost (Newton boosting decision tree)
- 正则项-对每颗树的复杂度进行惩罚：相对于GBDT不容易过拟合
- 树的正则化：树的深度，树的叶节点数量，内部节点个数，叶节点分数
- XGBoost的正则化项有：叶节点数量，和叶节点分数
- 增益：ID3-信息增益，C4.5-信息增益比，CART-Gini系数
- XGBoost: 利用了二阶泰勒公式的相关结果，定义了新的打分函数 (选取特征)
- 树节点分裂方法：对选取的节点的值进行分裂方法：近似方法--根据特征的样本个数选取某一些分位数作为分裂点的值
- 稀缺值的处理
- 显式地将树模型的复杂度作为正则项加在优化目标
##### XGBoost 正则化项
- 怎样使用的正则项(巧妙的将正则项融入树的构造，从损失函数一步一步的推导出树构造时的评分函数-使用了二阶相对精确，以叶节点来计算新的评分函数)
- 正则化包含了叶节点的个数和每个叶节点的值的L2正则化项
##### XGboost 并行处理
[BLOG](http://zhanpengfang.github.io/418home.html)
- 对每一个level 的 node splitting 进行并行化同时处理：由于每个节点包含的样本数量是不一致的，所以每个节点的处理速度是不一样的
- 在处理feature(选择)时进行并行化，针对某个feature然后进行每个分裂点并行
- 提前将样本进行排序，产生feature spliting然后并行化每个feature spliting

##### XGBoost 自定义损失函数
允许用户定义自定义优化目标和评价标准

##### XGBoost 缺失值处理方法
[怎么理解决策树、xgboost能处理缺失值？而有的模型(svm)对缺失值比较敏感呢?](https://www.zhihu.com/question/58230411)
[xgboost是如何处理缺失值的](https://www.jianshu.com/p/a9f49e08be86)
[决策树处理缺失值](https://blog.csdn.net/u012328159/article/details/79413610)

##### XGBoost 剪枝的方法
分裂时遇到一个负损失时，GBM会停止分裂。因此GBM实际上是一个贪心算法。
XGBoost会一直分裂到指定的最大深度(max_depth)，然后回过头来剪枝。如果某个节点之后不再有正值，它会去除这个分裂。
这种做法的优点，当一个负损失（如-2）后面有个正损失（如+10）的时候，就显现出来了。GBM会在-2处停下来，因为它遇到了一个负值。但是XGBoost会继续分裂，然后发现这两个分裂综合起来会得到+8，因此会保留这两个分裂

##### XGBoost内置交叉验证方法可以选择合适的参数
GBM不能，使用的Grid search 选择少量的参数

#### XGBoost参数调节
三类参数：通用参数，Booster参数，学习目标(训练函数)参数
基模型的复杂度：gama(最小的增益),max_depth,min_child_weight,
随机化抽样: subsample,col_sample,
正则化项：alpha,lambda
[参数调节详细介绍](https://blog.csdn.net/han_xiaoyang/article/details/52665396)
[xgboost参数介绍](https://blog.csdn.net/zc02051126/article/details/46711047)
[GBM参数调节](https://blog.csdn.net/han_xiaoyang/article/details/52663170)
[xgboost参数调节-知乎](https://zhuanlan.zhihu.com/p/52501965)
[xgboost-调参-知乎](https://zhuanlan.zhihu.com/p/35061092)
eta(缩减因子 避免过拟合),

#### lightGBM
- 直方图算法


### Stacking (模型融合方法)
[博客,比较详细解释了stacking](https://blog.csdn.net/maqunfi/article/details/82220115)
[stacking原理图](https://blog.csdn.net/zkx1949/article/details/83273914)

ID3
- 信息增益
- 信息熵
- 经验熵
- 条件熵
- 倾向于选择属性值较多的特征进行分裂

C4.5
- 信息增益率
- 分裂信息

CART
- 平均误差
- 纯度
- 基尼指数


XGBoost比GBDT
- 使用二阶泰勒
- 使用正则项: 叶子节点的个数,叶子节点值的
- 使用shrinkage (学习率,降低每颗树的拟合精度)
- 列抽样
- 忽略缺省值，自动处理缺省值
- block结构
- 并行化
- 节点分裂的近似算法

lightGBM
- 使用直方图算法--降低了内存, 加快了节点分裂的计算
- 使用直方图作差加快计算
- 使用leaf-wise的方法而不是level-wise构建树
- 特征并行: 每个worker在其的特征集上作划分(每个worker都有数据)，然后合并每个worker的划分选择最好的
- 数据并行：每个worker作直方图，然后不同的worker合并直方图
- 能直接使用类别

调参
XGBOOST
- eta (shrinkage)
- lambda
- alpha
- gama
- max_depth
- max_leaf_nodes
- min_child_weight

lightGBM
- lambda
- min_leaf_gain
- num_leafs
- max_length
- min_data_in_leaf
- min_sum_hessian_in_leaf
- max_bin
- feature_fraction
- bagging

先调树然后调正则项

调参步骤: 大的学习率(learning_rate)-->基树(num_leaves,max_depth,max_bin)-->min_data_in_leaf-->feature_fraction+bagging_fraction-->alpha,lambda-->小的learning_rate进行最后的学习


随机森林调参
[博客](https://blog.csdn.net/xuxiatian/article/details/54410086)
- criterion(基尼或者信息增益)
- max_depth：不做限制时会分裂到只剩一个
- max_leaf_nodes
- min_sample_split(能分裂的最少样本数)：样本量大时最好进行设置
- min_smaple_leaf(叶子节点最少样本数)：特征值不多不加限制,特征值多的话加以限制
- max_features: 特征不多时使用None(所有特征)
- min_impurity_split划分的阀值


决策树的剪枝
- C4.5 ID3的剪枝方法 用总体损失函数 = 经验损失+lambda正则项(叶子节点的个数)
- CART的剪枝方法
