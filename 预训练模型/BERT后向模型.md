改进
[nlp中的预训练语言模型总结(单向模型、BERT系列模型、XLNet)](https://zhuanlan.zhihu.com/p/76912493)

[站在BERT肩膀上的NLP新秀们(PART I)](https://zhuanlan.zhihu.com/p/68295881)
[站在BERT肩膀上的NLP新秀们（PART II）](https://zhuanlan.zhihu.com/p/68362016)
[站在BERT肩膀上的NLP新秀们（PART III）](https://zhuanlan.zhihu.com/p/69700975)

[Recent Highlights of BERTs（一）](https://zhuanlan.zhihu.com/p/76724992)
[Recent Highlights of BERTs（二）](https://zhuanlan.zhihu.com/p/76747316)
[Recent Highlights of BERTs（三）](https://zhuanlan.zhihu.com/p/76780489)

[NLP的bert和后bert时代的挑战](https://zhuanlan.zhihu.com/p/66896856)
"""
1.介绍了额外的数据与任务数据共同使用来训练模型的方式;
"""
[后Bert时代NLP相关进展](https://zhuanlan.zhihu.com/p/67314622)


微调相关
[BERT fintune 的艺术](https://zhuanlan.zhihu.com/p/62642374)



