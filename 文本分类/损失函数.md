### 1 关于损失函数应该学习的知识点
- 各种损失函数数学形式
- 各种损失函数的优缺点
- 交叉熵损失函数可以解决输出层参数学习缓慢的问题(原因是在对输出层的导数没有了sigmoid的导数了)
- 有些损失函数其实都有其特定的概率解释(两件事，最大似然估计，最大后验估计)
- 类别权重和样本权重
- 损失函数的值越低不代表最后的评价指标越好


### 总
对数损失（log 损失): 逻辑回归
二分类交叉熵损失
- 归一化可以用softmax也可以用sigmoid
多分类交叉熵损失
- 归一化用softmax
hinge损失
- 二分类
- 多分类

### 优秀的博客资源
[机器学习中的目标函数、损失函数、代价函数有什么区别？](机器学习中的目标函数、损失函数、代价函数有什么区别？)
- 第一个回答(评论)通俗易懂的讲清楚了这三个关系
[Tensorflow 中的损失函数 —— loss 专题汇总](https://zhuanlan.zhihu.com/p/44216830)
- 讲了tensorflow中损失函数的使用，以及各种损失函数的优缺点。
[外文博客总结了常用的regression损失函数](https://heartbeat.fritz.ai/5-regression-loss-functions-all-machine-learners-should-know-4fb140e9d4b0)
[pytorch](https://pytorch.org/docs/stable/nn.html#loss-functions)
[tensorflow]()
[博客](https://www.cnblogs.com/lliuye/p/9549881.html)
[逻辑回归和softmax的比较](https://blog.csdn.net/hahaha_2017/article/details/81066673)
[blog 全](http://www.csuldw.com/2016/03/26/2016-03-26-loss-function/)
[对比了交叉熵损失和均方损失前者解决了输出层参数学习缓慢的情况](https://zhuanlan.zhihu.com/p/37217242)
[损失函数与概率论之间的关系](https://zhuanlan.zhihu.com/p/33568166)
[博客1](https://blog.csdn.net/wodemimashi125/article/details/82421484)
[tensorflow 多分类log损失的实现](https://stackoverflow.com/questions/53027819/tf-losses-log-loss-for-multiply-classes)
[hinge损失](https://www.cnblogs.com/guoyaohua/p/9436237.html)
[怎么理解surrogate loss function代理损失函数？](http://sofasofa.io/forum_main_post.php?postid=1000605)
[PyTorch 学习笔记（六）：PyTorch的十七个损失函数](https://zhuanlan.zhihu.com/p/61379965)


### 逻辑回归损失和交叉熵的关系
从对数损失推导逻辑回归，用标签{-1,1}然后用sigmoid定义正向的概率P，则负项的概率就是1-p。对于sigmoid函数来说f(-x)=1-f(x)。所以对于各个样本的概率可以统一写成f(ys)我们的优化目标就是最大似然函数，由于最大似然不好求，所以改用log对数最大似然，再取负数就是最小化了。
从交叉熵推导，假设真实分布为(0,1)然后用sigmoid求概率，就可以定义交叉熵损失了。

### 术语
[似然函数 百度百科](https://baike.baidu.com/item/%E4%BC%BC%E7%84%B6%E5%87%BD%E6%95%B0/6011241?fr=aladdin)
用来确定参数，即参数在取什么值的情况下，能使获得的样本的发生的概率最大。即将多个样本的概率的乘积最大。
[最大似然估计](https://baike.baidu.com/item/%E6%9C%80%E5%A4%A7%E4%BC%BC%E7%84%B6%E4%BC%B0%E8%AE%A1)
