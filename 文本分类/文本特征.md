## 0.预处理
分词(中文)
tokenization
根据词频进行过滤
根据文档频率

## 1.文本统计特征
- 文本长度
- 单词总数
- the number of characters
- the number of upper characters
- the rate of upper characters
- the number of words
- the number of unique words
- the rate of unique words


## 2.语法语义特征
- PosTAG: One hot


# 2.向量空间模型特征表示
无监督的学习方法:将文本映射到向量空间,这些向量空间可以用来计算文本之间的相似度然后做文本的聚类.

## 2.1.传统特征

**所有的特征提取都是基于词频(term frequency)和文档频率(document frequency)**

### 2.1.1 词袋模型(TF)
以单词为坐标,形成一个向量,统计每个词出现的次数作为该词的的值
优点: 简单快读
缺点: 1. 维数大;2.没有考虑词与词之间的关系(顺序性,共现性);3.常用词影响很大

### 2.1.2 n-grams(n-元语法词袋模型)
改进点: 考虑了词的局部顺序性
缺点: 进一步加大了维数,维数过于大

### 2.1.3 TF-IDF
- 一种简单的组合TF和IDF的方式, 提取出来的特征不具备类别区分度
[TF-IDF用于文本分类](https://www.jianshu.com/p/edad714110fb)
为什么能用此特征: 表征了某个词对于构成一篇文档的重要程度,即该词是形成该篇文档所独一无二的词,还是一些普通常见,所有文档都具有的词。总和表征了词在文档中的重要程度(IF)和文档的区分程度(IDF)
优点: 设置了每个单词的权重
特征用途：1. 提取关键词(计算出IF-IDF后进行排序选择)；2. 用于信息检索(计算搜索词的TF-IDF然后求和得到它们在每篇文档中的IF-IDF选择IF-IDF最高的作为我们返回的检索信息)；3.用于向量空间模型
缺点：1.没有考虑词的顺序性,相关性;2.没有考虑类间文档的词的分布(即那些特征在类间文档是有差异的, 类间差异要大);3.没有考虑类内的文档的分布,即类内文档该词是否分布均匀 (类内文档分布要相似).; 4. 对应于2、3即表示TF-IDF只考虑了文档间的分布

### 2.1.4 LDA
...
### 2.1.5 LSI
..
## 2.2 现代文本特征向量
### 2.2.1 word2vec

### 2.2.2 glove

### 2.2.3 fasttext


# 3.特征处理
一个文档的特征为一个特征向量feature = [x1,x2,...,xn]，n很大会造成维度灾难
目的: 减少维度, 怎么做呢
基于词袋模型/n-grams/IF-IDF表示的特征向量维度很大(特征很多,我们要从中找出一些与任务有关--分类任务--的特征,这就用到了特征选择)

## 3.1 特征选择(feature selection)
从特征向量feature = [x1,x2,...,xn]中选择k个特征作为新的特征feature = [x1,x2,...,xk]
对于文本,每个特征项是文本中的单词,单词具有语义信息,为了保留语义信息,常常是选择与分类有关的K个特征(k个单词)作为文本的特征
[参考博客](https://blog.csdn.net/Class_guy/article/details/81175328)
### 3.1.2 信息增益
有特征t时与无特征t时的混乱程度, 有特征t时
注意这里的计算方法
[参考博客](https://zhuanlan.zhihu.com/p/23199165)
[参考博客](https://www.iteye.com/blog/hxraid-767364)
[信息增益提取文本分类特征词代码](https://blog.csdn.net/m11061003/article/details/85845805)
- 表示该特征能为文本分类带来多少信息,带来的信息越多那么越助于文本分类
- 关键概念：信息熵(正)，条件熵
- 判别, 对于特征条件熵越小越好
- 往往是特征分布的类别不叫均匀时比较大
即有了特征t后对分类的帮助,在特征t的帮助下,系统的混乱程度
缺点: 只能获得全局特征，每个类别都有的特征，不能获得每个类别其独有的特征，能获得整个训练集的特征, 但是不能获得在不同类别间的差异程度,权重的计算

### 3.1.2 卡方检验
有特征t与无特征t时，类别在样本中的分布是一样的
原假设(特征与类别分类不相关)，备择假设，差值衡量公式
计算过程: 计算每个特征的卡方值，然后从大到小对特征进行排列,选择前面的几个特征作为我们要用的特征
缺点: 计算时，只考虑该词是否在文档中出现，而不考虑出现了多少次，将低频词与高频词的作用看成是一样的，所以夸大了高频词的权重

## 3.2 特征抽取(Feature extraction 降维)
将特征向量feature=[x1,x2,x,...,xn]降维到另外一个空间
