---
title: 我理解的高斯贝叶斯分类器
categories:
- machine learning
tags:
- machine learning
- gaussion baye
- classification
date: 2019-03-23 12:55:00
---

动机：在朴素贝叶斯中，我们假设的输入变量都是属于类别变量。但是实际问题中，不仅仅是类别变量，还有实值变量。所以此时就要用新的方法，来处理输入变量是实值变量的情形。

# 1 输入变量全为实值
假设我们有数据集$D=\{(\bold{X}_1,y_1),(\bold{X}_2,y_2),...,(\bold{X}_R,y_R)\}$,输入变量$\bold{X}=\{X_1,X_2,...,X_n\}$里面的每一个特征都为一个实值数，输出变量$Y$为类别变量可取的类别数为$K$。

现在我们的问题是输入一个$X$，我们判断它的类别。怎样判断？用贝叶斯判断器
$$
\begin{aligned}
y = \underset{Y}{\arg\max} \ \ P(y|X)
\end{aligned}
$$
但是我们并不知道$(y|X)$这个条件分布。所以进行转换
$$
\begin{aligned}
y &= \underset{Y}{\arg\max} \ \ P(y|X)\\
&=\underset{Y}{\arg\max} \ \ \frac{P(y,X)}{P(X)}\\
&=\underset{Y}{\arg\max} \ \ \frac{P(X|y)P(y)}{P(X)}
\end{aligned}
$$
由于我们给出一个$X$的值，它就是确定的。所以$X$的概率密度并不会影响到我们最后的判断，即
$$
\begin{aligned}
y = \underset{Y}{\arg\max} \ \ P(X|y)P(y)
\end{aligned}
$$
这是典型的用先验概率来估计后验概率，将$Y$表示为病因，将$X$表示症状，现在发现了症状要知道它的病因。即用病因的先验概率计算。

所以现在问题转换成了求$(X|y),(Y)$两个分布。但是我们不知道啊，所以引入假设。

假设样本间独立同分布(i.i.d.)。输出变量$Y$服从多项分布，即
$$
\begin{aligned}
Y \sim Multinomial(p_1,p_2,...,p_K)
\end{aligned}
$$
输入变量的条件概率分布$(X|Y)$服从多维高斯分布
$$
\begin{aligned}
(X|Y)\sim N(\bold{\mu}(y),\bold{\Sigma}(y))
\end{aligned}
$$
均值向量和协方差矩阵都为$Y$的函数。

有了这两个分布的假设后，我们接下来就是求这两个分布。

对于分布$Y$直接用极大似然估计进行求解
$$
\begin{aligned}
p_k=\frac{\#\{i ;y_i=k\}}{R}
\end{aligned}
$$


对于$(X|Y)$同样用极大似然估计方法进行求解。即在$Y$的各个类别中单独求解,比如在第k类中
$$
\begin{aligned}
\bold{\mu}_k,\bold{\Sigma}_k
\end{aligned}
$$

由高斯分布的极大似然估计我们可以求得一下结果
$$
\begin{aligned}
\bold{\mu}_k&=\frac{1}{\#\{i;y_i=k\}}\sum_{i,y_i=k}\bold{x}_i\\
\bold{\Sigma}_k&=\frac{1}{\#\{i;y_i=k\}}\sum_{i,y_i=k}(\bold{x}_i-\bold{\mu}_k)(\bold{x}_i-\bold{\mu}_k)^T
\end{aligned}
$$

所以最后的贝叶斯分类器为
$$
\begin{aligned}
y = \underset{Y}{\arg\max} \ \ \frac{1}{(2\pi)^{n/2}||\bold{\Sigma}_k||^{1/2}}\exp \left( -\frac{1}{2}(\bold{x}-\bold{\mu}_k)\bold{\Sigma}_k^{-1}(\bold{x}-\bold{\mu}_k)^T \right)P_k
\end{aligned}
$$


## 1.1 缺陷
- 当$n$很大时，$R$很小时，我们求出的协方差矩阵$\bold{\Sigma}_k$就是个奇异阵，不存在逆。
- 因为每一个类别中都要求一次协方差矩阵，而协方差矩阵中独立的变量数很多，所以我们要求的参数就很多，$O(Km^2)$。
- 过拟合问题。因为我们要求解的参数多，当样本数量不足够大时，学习的过程就仅仅是记住当前的训练样本的特征。

## 1.2 解决办法
降低协方差矩阵中的参数个数
- $Y$的各个类别中$X$的协方差矩阵都取全局的协方差矩阵。此时对应的是线性判别方法(LDA)。
- 属性因子之间的独立性假设，即$P(X|Y)=P(X_1|Y)P(X_2|Y),...,P(X_n|Y)$，此时协方差举证就简化为一个对角矩阵。

备注：通常我们在机器学习中会涉及到两个$(i.i.d.)$一是对各个样本之间的假设，二是对输入变量的各个属性因子之间的假设。

# 2 输入变量为类别实值混合型
假设我们有数据集$D=\{(\bold{X}_1,y_1),(\bold{X}_2,y_2),...,(\bold{X}_R,y_R)\}$。输入变量为$\bold{X}=\{u_1,u_2,...,u_q,v_1,v_2,...,v_{n-q}\}$其中$(u_1,u_2,...,u_q)$为实值变量，$(v_1,v_2,...,v_{n-q})$为类别变量，输出变量$Y$为类别变量可取的类别数为$K$。

同样，此时有输入变量X的观察值，要求输出变量的类别，可以表达为贝叶斯公式
$$
\begin{aligned}
y &= \underset{Y}{\arg\max} \ \ P(Y|X)\\
&=\underset{Y}{\arg\max} \ \ P(Y|(\bold{u},\bold{v}))\\
&=\underset{Y}{\arg\max} \ \ P((\bold{u},\bold{v})|Y)P(Y) \\
&=\underset{Y}{\arg\max} \ \ P(\bold{u}|\bold{v},Y)P(\bold{v}|Y)P(Y)
\end{aligned}
$$
接下来的任务就是求解$P((\bold{u},\bold{v})|Y),P(Y)$。对于$P(Y)$可以直接根据极大似然估计，按照每一个类别出现的次数进行估计计算。而$P((\bold{u},\bold{v})|Y),P(Y)$为$Y$的条件概率，所以我们需要计算在每一个$Y$的类别下的条件概率分布。比如说求第$k$类,应用链式法则
$$
\begin{aligned}
P((\bold{u},\bold{v})|y_k)=P((\bold{u})|\bold{v},y_k)P(\bold{v}|y_k)
\end{aligned}
$$
对于$P(\bold{v}|y_k)$即求第$k$个类别下，类别变量的连个概率分布。求着概率分布有两个方法，第一个就是直接求解，第二个就是用Naive方法(类别因子之间是独立的)。

## 2.1 joint-Gaussion方法
$$
\begin{aligned}
P(\bold{v}^j|y_k)=\frac{\# \{i;v_i=v^j \ and \ y_i=y_k\}} {\#\{i;y_i=y_k\}}
\end{aligned}
$$

对于$P((\bold{u})|\bold{v},y_k)$，可以假设该条件概率分布为高斯分布，然后用极大似然估计进行求解
$$
\begin{aligned}
\mu_{\bold{v}_j,y_k}&=\frac{1}{\# \{i;v_i=v^j \ and \ y_i=y_k\}}\sum_{v_i=v^j \ and \ y_i=y_k}(\bold{u_i})\\
\Sigma_{\bold{v}_j,y_k}&=\frac{1}{\# \{i;v_i=v^j \ and \ y_i=y_k\}}\sum_{v_i=v^j \ and \ y_i=y_k}(\bold{u_i-\mu_{\bold{v}_j,y_k}})(\bold{u_i-\mu_{\bold{v}_j,y_k}})^T
\end{aligned}
$$

当给出了$\bold{X}=(\bold{u},\bold{v})$时，对$Y$的预测可以写为
$$
\begin{aligned}
y &= \underset{K}{\arg\max} \ \ P(\bold{v}|y=k)P(\bold{u}|\bold{v},y=k)P(y=k)\\
P(\bold{u}|\bold{v},y=k)&=\frac{1}{(2\pi)^{1/q}||\Sigma_{\bold{v},y=k}||^{1/2}}\exp \left( -\frac{1}{2}(u-\mu_{\bold{v},y=k})^T\Sigma_{\bold{v},y=k}^{-1}(u-\mu_{\bold{v},y=k}) \right)
\end{aligned}
$$

但是这样求解时存在一个问题，因为用到了用joint方法求解，当把类别输入变量进行组合时，会有很多种组合，当数据量不足时，这是严重估计不足的，造成overfitting。所以应该考虑其他求解方法。用naive方法。


## 2.2 Naive-Gaussion方法
由于上面用到的joint方法存在严重不足，所以常常假设输入变量的各个属性因子之间是独立的。同时对于实值变量是高斯分布，类别变量是多项分布。此时就可以用Naive的方法进行估计了。同样是对每一个y的类别下进行估计的。
$$
\begin{aligned}
P(\bold{u},\bold{v}|Y)=\prod_{i=1}^{q}P(u_i|Y)\prod_{j=1}^{n-q}P(v_j|Y)
\end{aligned}
$$
其中假设每一个$P(u_i|Y)$服从高斯分布$N(\mu_i,\sigma_i)$,而对于每一个$P(v_j|Y)$服从多项分布$Multinomial[p_{j,1},p_{j,2},...,p_{j,n-q}]$。然后应用极大似然估计分别求它们。
$$
\begin{aligned}
\mu_{k,i}&=\frac{1}{\# \{j;y_j=k\}}\sum_{j;y_j=k}u_{j,i}\\
\sigma_{k,i}&=\frac{1}{\# \{j;y_j=k\}}\sum_{j;y_j=k}(u_{j,i}-\mu_{k,i})^2\\
p_{k,i}&=\frac{\# \{j;y_j=k\ and\ v_j=i\}}{\# \{j;y_j=k\}}
\end{aligned}
$$
然后用贝叶斯分类器确定$Y$的类别。

# 3 总结
- 当输入变量都是类别变量时，我们可以简单的使用朴素贝叶斯方法进行求解。
- 当输入变量都是实值变量时，我们可以用高斯(Gaussion)贝叶斯方法进行求解。
- 当输入变量即含有类别变量，又含有实值变量时，我们可以用joint-Gaussion方法求解，或者Naive-Gaussion方法求解，但是第一种方法求出来的结果不可靠，通常用第二种方法进行求解。

另外，由于协方差矩阵的参数很多，所以为了简化求解的过程，我们通常化进行一些假设，进而简化求解过程。比如线性判别器。

reference
[1] 李航 《统计学习方法》
[2] 周志华 《机器学习》
[3] PPT from Andrew W. Moore in CMUhttp://www.cs.cmu.edu/~awm/tutorials

