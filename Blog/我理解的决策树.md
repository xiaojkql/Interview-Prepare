---
title: 我理解的决策树
categories:
- machine learning
tags:
- decision tree
- classification
- regression
date: 2019-03-22 11:00:00
---

# 1 预备知识
## 1.1 熵的定义与解释
假设随机变量A的分布为下图1所示，共有四个取值A，B，C，D。现在我们要将A,B,C,D按照图2的形式进行编码进行二进制编码，那么这四者中每一个平均需要多少个bit呢？

$$ 
\begin{alignedat}
bbits &= 1\times\frac{1}{2} +2\times\frac{1}{4} + 3\times\frac{1}{8}+3\times\frac{1}{8} \\
&=-\frac{1}{2}\log(\frac{1}{2})-\frac{1}{4}\log(\frac{1}{4})-\frac{1}{8}\log(\frac{1}{8})-\frac{1}{8}\log(\frac{1}{8})
\end{alignedat}
 $$

<center>

![图1 随机变量X的分布律](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190319164254.png)
</center>

<center>

![图2 编码方式](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190319165556.png)
</center>

**定义1:** 对于一个随机变量(离散)，它的的分布律为$P\{X=x_i\}=p_i$,则它的熵为(单位为bits)：
$$ H(X)=\sum_{i=1}^{m}-p_i\log(p_i) $$
当$m=2$时，将第一个变量的概率定义为$p$，则此时熵为$H(X)=-p\log(p)-(1-p)log(p)$,$H$关于$p$的图像为  
很明显，当$p=0.5$时上$H$最大，当$p$等于两个极端时，$H$最小。那么当$p=0.5$时，表示随机变量$X$的两种情况都有可能发生，发生的概率几乎差不多。反之，当$p$接近于0或者1时，此时随机变量取两个可能值的概率之间就存在差距。  
所以熵$H$与随机变量X之间的关系就为：  
$H$越大，则表示$X$取到它所有可能值的概率相等,X是一个比较平稳的分布  
$H$越小，则表示$X$取到它所有可能值的概率不均衡，即是一个峰值分布，一些值概率很大，而其他则很小  
<center>

![图3 熵H与X分布的关系](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190319171758.png)
</center>

## 1.2 分类误差率、熵和概率之间的关系
如下图所示，是二分类中，分类误差率和熵随着其中某个类别的概率之间的关系，由该图可知，当概率$p—>0 或者 1$时，分类误差率是很小的，当$p$在0.5附近时，分类误差率是很大的。  
直观的解释就是当p趋近于0或者1时，此时某一情况发生的概率远远大于另一种情况。当然，在给我们一个样本时，我们选择概率最大的类，作为该样本的类，我们的错误很小了。反之，当p在0.5附近时，两种情况发生的概率相等，那么我们该把样本归为哪一类呢？所以此时分类错误率是很高的。  
图中熵与概率之间的关系完全与分类误差率和概率之间的关系一样。所以这就告诉我们可以用熵来描述当前的分类误差率！
<center>

![三者之间的关系](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190319172429.png)
</center>

# 2 决策树
## 2.1 决策树的直观感觉
给定一个训练数据集$train$, 给一个样本$test$,我们要该测试样本进行分类，那么我们该怎样分类呢？很直接的方法就是，计算训练数据集中每一个类别$y_i$的概率，然后选择其中概率最大的那个$y_i$作为测试样本的类别。由前面我们知道，当分类类别$Y$的概率分别很均匀时，我们的分类错误率很高，只有当$Y$的概率分布很极端时，我们的分类错误率才较低。  
那么问题来了？当$Y$分布很均匀的时候，我们该怎样降低分类错误率呢？  
对训练数据集进行分堆，且每一堆中$Y$的类别很一致，即Y的分布很极端。此时来了一个测试数据，我们选择一个堆，然后再对该测试数据集进行分类，此时分类错误率就会降低了吧。那么如何对训练数据集分堆呢？  
因为$Y$的类别很显然受到它的属性决定，每一个属性取不同的值都有可能导致$Y$的类别不同，所以就选择其中的一个属性按照它的可能取值，对该训练集进行分堆。
## 2.2 分堆的规则

## 2.3 三种学习算法的对比

## 2.4 cart学习算法的剪枝理解

<center>

![](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190319174841.png)
</center>
