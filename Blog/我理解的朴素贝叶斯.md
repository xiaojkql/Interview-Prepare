---
title: 我理解的朴素贝叶斯
categories:
- machine learning
tags:
- machine learning
- naive bayes
- categories input
- classification
date: 2019-03-22 17:25:00
---

# 1 变量描述
有一个数据集包含了$\{(X_1,X_2,...,X_n,Y)_1,(X_1,X_2,...,X_n,Y)_2,...,(X_1,X_2,...,X_n,Y)_1\}$。假设定义一个变量为$W=(X_1,X_2,...,X_n,Y)$为一个随机事件，即有一个概率分布对应着W。这个概率分布也即$n+1$变量$X_1,X_2,...,X_n,Y$的联合概率分布$D_{X,Y}$。

给定了这些数据，我们就可以用概率密度估计的方法估计它们的联合概率分布。我们可以用Joint Density Estimator求他们的联合概率分布。但是我们不能用*Naive Density Estimator*。原因如下。

因为这里我们假设$Y$为我们的输出变量，$X_1,X_2,...,X_n$为我们的输入变量。它们之间存在着某种函数关系$Y=f(X_1,X_2,...,X_n)$。由于这层函数关系的存在，$X_1,X_2,...,X_n,Y$这$n+1$个随机变量之间是不独立的，所以是不能用Naive的方法来求它们的联合概率分布。此时我们该怎么办呢？

因为有这个Y和f的存在导致了这些变量的不独立，所以不能用naiv的方法来求联合概率分布。如果我们将$Y,f$踢出去，并假设$X_1,X_2,...,X_n$是独立的。那么，此时变量$X_1,X_2,...,X_n$之间的联合概率分布就可以用Naive的方法求出，因为它们之间是独立的。因为有$Y,f$才能构成我们的问题，所以我们必须要引入$Y,f$。怎么办呢？

用条件概率分布，即$(X_1,X_2,...,X_n|Y)$。此时变量为$(X_1,X_2,...,X_n)$，条件为$Y$，因为这些条件分布中的变量是独立的，所以此时又可以用Naive方法了。即
$$ P(X_1,X_2,...,X_n|Y)=\prod_{i=1}^{n}P(X_i|Y) \tag{1}$$

# 2 朴素贝叶斯方法思想
朴素贝叶斯算法是在解决分类问题，即输入一些属性因子，求这些属性因子的类别。

最简单的朴素贝叶斯是在解决一个输入输出变量都是一些类别变量的分类问题。即每一个输入变量$X_i$和输出变量$Y$都是类别变量。

对于分类问题，我们知道用贝叶斯分类器可以表达为求最大后验概率
$$
\begin{aligned}
y = \underset{Y}{\arg\min} \ \ P \left( Y|(x_1,x_2,...,x_n) \right)
\end{aligned}
$$
应用条件概率公式和链式法则有：
$$
\begin{aligned}
y &= \underset{Y}{\arg\min} \ \ P \left( y|(x_1,x_2,...,x_n) \right)\\
&=\underset{Y}{\arg\min} \ \ \left(\frac{P \left( y,(x_1,x_2,...,x_n) \right)}{P(x_1,x_2,...,x_n)}  \right)\\
&=\underset{Y}{\arg\min} \ \ \left(\frac{P \left( (x_1,x_2,...,x_n)|y \right)P(y)}{P(x_1,x_2,...,x_n)}  \right)
\end{aligned}
$$
因为对于每一个具体的输入$(x_1,x_2,...,x_n)$，它的概率是故意的所以上式中的分母项对于每一个$y$都是一致的。所以问题转换为
$$
\begin{aligned}
y &=\underset{Y}{\arg\min} \ \ P\left( (x_1,x_2,...,x_n)|y \right)P(y)
\end{aligned}
$$

现在问题转换为求解$P\left( (x_1,x_2,...,x_n)|y \right)$和$P(y)$。对于前者，因为此条件概率分布的各个变量具有独立性，用naive方法进行求解
$$
\begin{aligned}
P(x_1,x_2,...,x_n|y)=\prod_{i=1}^{n}P(x_i|y)
\end{aligned}
$$
所以转换为求解$P(x_i|y)$同样应用边缘分布的原则进行求解。

对于$P(y)$，将此看做联合概率分布$D _{X,Y}$的边缘分布。

# 3 求解边缘分布
对于$P(x_i|y)$和$P(y)$两个分布的求解，我们可以用极大似然估计或者贝叶斯估计(最大后验概率估计)进行求解。

# 4 总结
朴素贝叶斯方法分解为"朴素"和"贝叶斯"两个部分，首先"朴素"体现在输入变量之间的独立性，方便了求解条件联合概率分布。"贝叶斯"体现在我们在求具体的分类时，还用到了先验概率$P(Y)$,即求出的$y$是最大后验概率对应下的$y$。

说明：可以将输入变量$X$看作是症状，因为我们可以观察到它，$y$看作是病因。现在是给定症状$X$求病因$y$，所以去求最大后验概率。

通常机器学习中，我们假设输入变量之间是独立的，而输入输出变量之间是不独立的。所以不能之间求输入输出变量的联合概率分布，而是要求给定输出变量下的条件概率分布。此时就可以用朴素的方法进行求解联合概率分布。

reference
[1] 李航 <<统计学习方法>>
