---
title: 我理解的线性回归
categories:
- machine learning
tags:
- machine learning
- regression
- linear
date: 2019-03-23 13:54:00
---

# 1 用概率进行推导

假设输入变量为$X=\{X_1,X_2,...,X_n\}$，输出变量为$Y$，一个函数$f$。假设输出变量与输入变量之间的关系为$Y=f(X)$。现在假设$X$为n维随机向量，$Y$为随机变量，$f$也为一个随机事件。假设它们之间联合概率分布为$P(X,Y,f)$。

现在假设我们知道输入变量$X$和函数$f$，要求输出变量$Y$的值。$Y$的条件概率是$P(Y|X,f)$。根据期望经验最小，我们要对Y进行预测，那么我们选择的$Y$应该是概率最大的，即求出的y应该是
$$
\begin{aligned}
y=\underset{Y}{\arg\max} \ \ P(Y|X,f)
\end{aligned}
$$
解释，因为我们要预测$Y$的值，$Y$有一个真实值$Y_{true}$。所以预测的期望损失为：
$$
\begin{aligned}
E(Y_{pred})&=\int I(Y_{pred}\ne Y_{true})P(Y,X,f)\\
&=\int I(Y_{pred}\ne Y_{true})P(Y|X,f)P(X,f)
\end{aligned}
$$
因为$X,f$已知，所以$P(X,f)$的值时固定的。此时只有$P(Y|X,f)$在变化，对整个期望误差有影响，为了使预测误差最小，我们在所有情况下预测Y的值为$P(Y|f)$最大的时候对应的$Y$值。

但是有一个问题，我们并不知道$P(Y|X,f)$的分布。怎么办？当然是假设。那假设为啥分布呢？当然是高斯分布，因为我们相信在$X,f$确定的条件下，$Y$的大部分内容时确定，只有小部分是不确定的。而这小部分就是一个白噪声引起的。

假设$(Y|X,f)$为高斯分布，因为是条件下，所以该分布的参数自然而然就与$X,f$有关了。$(Y|X,f)$的均值为$f(X)$，方差为$\sigma^2$。所以：
$$
\begin{aligned}
P(Y|X,f)=N(f(X),\sigma^2)=\frac{1}{\sqrt{2\pi}\sigma}e^{-\left( \frac{(Y-f(X))^2}{2\sigma^2} \right)}
\end{aligned}
$$

现在我们获得了一些数据集$((x_1,y_1,f),(x_2,y_2,f),...,(x_R,y_R,f))$，我们想通过这些数据集来求$Y$的分布函数。用什么方法呢？极大似然估计，即这些样本出现了那么这些样本在原来的分布下的概率最大即：
$$
\begin{aligned}
\theta=\underset{\theta}{\arg\max} \ \ P((x_1,y_1,f),(x_2,y_2,f),...,(x_R,y_R,f))
\end{aligned}
$$
假设样本间是独立同分布的，所以
$$
\begin{aligned}
P((x_1,y_1,f),(x_2,y_2,f),...,(x_R,y_R,f)) &= P(x_1,y_1,f)\times P(x_2,y_2,f)\times,...,\times P(x_R,y_R,f)\\
&=P(y_1|x_1,f)\times P(x_1,f)\times P(y_2|x_2,f)\times P(x_2,f)\times\\&\ \ \ \ \ \ \,...,\times P(y_R|x_2,f)\times P(y_R|x_R,f)
\end{aligned}
$$

因为该问题是一个在给定$X,f$下预测$Y$的值，即在给定$X,f$下它们的概率是固定已知的，所以仅仅考虑的是$P(Y|X,f)$而由前面的定义该分布是一个高斯分布。所以极大似然估计转换为：
$$
\begin{aligned}
\theta &= \underset{\theta}{\arg\max} \ \ P(y_1|x_1,f)\times P(y_2|x_2,f),...,\times P(y_R|x_2,f)\\
&=\frac{1}{\sqrt{2\pi}\sigma}e^{-\left( \frac{(y_1-f(x_1))^2}{2\sigma^2} \right)} \times \frac{1}{\sqrt{2\pi}\sigma}e^{-\left( \frac{(y_2-f(x_2))^2}{2\sigma^2} \right)},...,\times \frac{1}{\sqrt{2\pi}\sigma}e^{-\left( \frac{(y_R-f(x_R))^2}{2\sigma^2} \right)}
\end{aligned}
$$

只要假定了函数$f$的一个具体形式，就有一组函数参数，带入上式，应用标准的极大似然估计流程便可以求出函数$f$。

# 2 用概率推导正则项
在上面求解中我们假设$f$是一个直接给出的，现在我们假设它也是一个随机的，即现在给出数据集$((x_1,y_1,f),(x_2,y_2,f),...,(x_R,y_R,f))$，我们需要求两个未知量即$Y，f$。应用条件概率最大：
$$
\begin{aligned}
Y,f=\underset{Y,f}{\arg\max} \ \ P(Y,f|X)
\end{aligned}
$$

我们应用链式法则对上式进行化简
$$
\begin{aligned}
Y,f&=\underset{Y,f}{\arg\max} \ \ P(Y,f|X)\\
&=\underset{Y,f}{\arg\max} \ \ \frac{P(Y,f,X)}{P(X)}\\
&=\frac{P(Y|f,X)P(f,X)}{P(X)}\\
&=\frac{P(Y|f,X)P(f|X)P(X)}{P(X)}\\
&=P(Y|f,X)P(f|X)
\end{aligned}
$$

此时，我们需要假设两个分布。同样，我们假设$=P(Y|f,X)$为高斯分布。如果假设$P(f|X)$为高斯分布,均值为0，那么此时用极大似然估计时，最后求出来的就是带有$l_2$正则项的回归，即岭回归。如果假设的是拉普拉斯分布，那么此时极大似然估计求出的就是拉索(lasso)回归。

**备注**：上面的f是一个通用的函数形式，我们在求解之前需要指定它的具体形式。当指定它的具体形式时，我们就得到了一系列参数。实际上我们对$(f|X)$的分布假设，是对$f$里面参数的分布假设。


# 3 用最小二乘进行推导
即用标准的监督机器学习建模方式：模型(model),参数(parameter),目标函数(Obj),损失函数(loss),正则项(Reg)几个要素分别处理。参数由模型决定。目标函数由损失函数和正则项共同构成。损失函数和正则项有参数决定。

首先，假设我们的模型是
$$
\begin{aligned}
Y_{pred}=f(X,\theta)
\end{aligned}
$$
其中$X$是输入变量，$Y$是输出变量，$\theta$是模型参数。

因为是在左预测任务所以用平方损失(MSE)作为损失函数
$$
\begin{aligned}
MSE&=(Y_{true}-Y_{pred})^2\\
&=(f(X,\theta)-Y_{true})^2
\end{aligned}
$$

正则项是参数$\theta$的函数
$$
\begin{aligned}
Reg = Reg(\theta)
\end{aligned}
$$

目标函数为
$$
\begin{aligned}
Obj(\theta) = (f(X,\theta)-Y_{true})^2+Reg(\theta)
\end{aligned}
$$
为了表示我们的拟合程度，我们要求目标函数在整个样本空间的均值要求最小
$$
\begin{aligned}
\theta&=\underset{\Theta}{\arg\min} \ \ \int Obj(\theta) \ P(Y,X)\\
&=\underset{\Theta}{\arg\min} \ \ \int ((f(X,\theta)-Y_{true})^2+Reg(\theta)) \ P(Y,X)\\
\end{aligned}
$$
因为我们并不知道(X,Y)的联合分布，所以就用我们已经得到的训练数据集((x_1,y_1),(x_2,y_2),...,(x_R,y_R))来替代。并假设这些训练数据集是同等概率的。所以，我们的目标函数变为在这整个训练数据集上均方损失
$$
\begin{aligned}
\theta&=\underset{\Theta}{\arg\min} \ \ \int Obj(\theta) \ P(Y,X)\\
&=\underset{\Theta}{\arg\min} \ \ \int ((f(X,\theta)-Y_{true})^2+Reg(\theta)) \ P(Y,X)\\
&=\frac{1}{R}\sum_{i=1}^{R}((f(x_i,\theta)-y_i^{true})^2+Reg(\theta))
\end{aligned}
$$


可以验证上面的极大似然估计求出来的目标函数与我们这里的目标函数等价。所以回归问题的均方损失是有概率解释的。即假设$(Y|X,f)$为高斯分布下，用极大似然估计求出来的。
