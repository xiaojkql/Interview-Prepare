# 1 什么是统计学习
假设有p维的输入变量变量$X_1,X_2,...,X_n$(ouput variables,predictors, independent variables, features)，对应着一个输出变量$Y$(output variables, response, dependent variable,),它们之间存在着某种关系：
$$ Y=f(X)+\varepsilon \tag{1}$$
其中$f$表示一些限定的但是形式未知的函数，$\varepsilon$表示随机项(噪声)，随机项与$X$之间是独立的且具有零均值，表示$Y$自己的变动。$f$表示由输入变量$X$提供给输出变量$Y$的系统信息。如下图所示，右边的蓝色曲线就代表$f$,垂直线就代表噪声。
<center>

![图1](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190321093323.png)
</center>
通常情况下函数$f$是不知道的，但是我们知道一些观察值，即有一个数据集$data={(X_1,Y_1),(X_2,Y_2),...,(X_n,Y_n)}$，此时我们的任务就是从这些观察的数据集找到函数$f$，即学习函数$f$。  
所以统计学习即是许多估计学习函数$f$的方法。

## 1.2 学习函数f的目的
我们学习函数$f$有两个目的，第一个是预测(prediction)，第二个是推测(inference)。
### 1.2.1 预测(prediction)
许多情况下，输入变量$X$的值时很容易获取的(简单的观测实验即可获取)，但是输出变量$Y$是很难获取的。但是，他们之间就存在某种函数关系，所以我们就用函数$f$来预测$Y$:
$$ \hat{Y}=\hat{f}(X)\tag{2} $$
这里$\hat{f}$表示函数$f$的估计，$\hat{Y}$表示对$Y$的估计。此时$\hat{f}$就是一个黑盒子，我们并不关心它的具体形式，我们只关心它对我们的$Y$的预测精度。  
预测值$\hat{Y}$对真实值$Y$之间的差距称为预测误差。它主要来源于两个方面，一种是可以降低的错误(reduccible error),另一个方面是不可降低的(irreducible error)。通常情况下，$\hat{f}$不是对$f$的精确估计，所以这就会引入了一些预测误差。这种原因引起的预测误差是可以降低的，因为我们可以使用最尖端的学习技术估计$f$。即使这种误差降低为0，我们的预测误差仍然存在，这是因为Y也与噪声$\varepsilon$相关。因此与噪声$\varepsilon$的变化也会影响预测误差。这种原因引起的误差是不可降低的，因为无论这是自然本质的，是没有办法解决的。  
$\varepsilon$表示了一些还没有包含在输入变量$X$中的一些引起$Y$变化的原因。  
假设在一个数据$X$，以及一个$\hat{f}$，那我们此时就可以预测$Y$:
$$ \hat{Y}=\hat{f}(X) $$
注意这里$Y$，$f$是有帽子的，$X$没有，因为前两者是估计的，但是后者是真实的。此时我们用平方损失的期望来计算我们的预测误差：
$$ E\{(Y-\hat{Y})^2\} =E\{(f(X)+\varepsilon-\hat{f}(X))^2\}\\
=E\{(f(X)-\hat{f}(X))^2\}+E\{\varepsilon^2\}+2E\{\varepsilon(f(X)-\hat{f}(X))\}\\=var\{(f(X)-\hat{f}(X))\} +var\{\varepsilon\}$$
其中$var\{(f(X)-\hat{f}(X))\}$表示预测值与真实的$Y$之间的差距，是可以通过使用尖端技术降低的，但是$var\{\varepsilon\}$是无法降低的，这是自然本质的。  
所以统计学习的任务就是找到尖端的技术降低$\hat{f}$的预测精度。  
### 1.2.2 推测(inference)
但是有时候不仅要对$Y$进行预测而且也需要知道$Y$与$X$之间的关系，此时$f$就不仅仅是黑箱子了，而要知道它的准确的函数形式。比如：
- 那一个输入变量对输出变量影响最大。(往往输出仅仅取决于少部分输入)
- 输入变量与输出是正相关还是负相关。

### 1.2.3 预测与推测之间的trade-off
**综上**，怎样学习$f$通常取决于我们的任务目标，是精确预测$y$就可以，还是知道输入与输出之间的关系，还是两者都需要。在不同的问题背景下，我们就要选择合适的学习$f$的方法。  
通常f越复杂，预测越精确，但是解释性就很差。
![](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190321104056.png)

## 1.2 怎样学习f
学习方法，建立模型分为两种形式，参数化的方法，和非参数化的方法。
**参数化方法**：  
分为两步
- 选择模型：定义一个函数形式，这个函数即可由一些参数(超参数)决定了，我们不在学习整个$p-$维的函数空间，而是学习由这些超参数决定的函数，即学习这些超参数。
- 学习算法，学习最好的超参数，即应用训练数据集进行拟合函数。
以模型为基础的学习方法就称为参数形式的方法，它降低了我们的学习难度，此时，只需要学习一些超参数即可。但是会导致一个问题，即模型选择的不好，会降低我们的拟合程度，可以选择更好的模型拟合，但是又会面临一个问题，即过拟合，将我们自然本质的变动因素$\varepsilon$也学习到了。
**非参数化方法**  
即不定义一个具体形式的$f$就进行学习。

# 2评估模型的精度
没有免费的午餐：不同的统计学习方法适合于不同的数据集，即针对不同的数据集我们应该选择合适的统计学习方法。但是该怎样选择模型呢？这就需要我们队模型进行评估。
## 2.1 评估拟合程度
给定一个数据集下，对于一个回归问题通常用均方损失MSE(mean square error)
$$ MSE = \frac{1}{n}\sum_{i=1}^{n}(y_i-\hat{f}(x_i))^2$$
当MSE用在训练数据集时，就称为训练误差，但是我们通常更在意的是MSE在测试数据集上的误差，即测试误差。所以，要选择测试误差越小的模型。通常在训练数据集上的误差会随着模型的flexibility增大而减小，但是在测试训练集上误差就会随着模型的flexibility增大先减小，然后再增大。  
在训练集上的误差很少，但是在测试集上的误差很大，就称为overfitting，这是由于学习方法太强大了，将一些随机噪声引起的pattern也学习到了。
<center>

![](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190321110430.png)
</center>
