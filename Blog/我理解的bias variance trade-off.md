---
title: 我理解的bias variance trade-off
categories:
- machine learning
tags:
- machine learning
- error
- bias variance
date: 2019-03-21 13:39:00
---

# 1 机器学习的误差
机器学习就是在给定数据集$data=\{(x_1,y_1),(x_2,y_2),...,(x_n,y_n)\}$学习下面的一个函数
$$ Y=f(X)+\varepsilon \tag{1}$$
其中$f$表示$Y$与$X$之间的函数关系，表示的是系统信息，$\varepsilon$表示的是随机误差。即我们在有输入变量$X$在系统关系作用下产生$Y$时,还要加上一个随机噪声项。这个噪声项是不可以学习的。  
将学习到的函数记为$\hat{f}$，有了这个关系以后我们就可以在给定X下求得$Y$的预测值$\hat{Y}=\hat{f}(x)$。学习当然是有误差的，用均方损失来计算误差可以表示为：
$$ 
\begin{alignedat}\ 
error&=E\{(Y-\hat{Y})^2\}\\
&= E\{(Y-\hat{f}(X))^2\}\tag{2}
\end{alignedat}$$
由于我们机器学习的主要目标就是学习一个函数$f$然后再做预测，所以我们不仅要求学习到的函数在训练数据集上表现的好，而且要求它在未知的测试数据集上也表现好，所以通常用测试误差来表示我们学习到的模型的好坏。所以我们在评估模型的时候就要考虑到其他数据，即全局数据，即这里的$error$表示的是在全局数据集上的误差,并求均值。对(2)式进行分解，并将(1)式带入，并应用均值函数的性质有：
$$ 
\begin{alignedat}\ 
error&= E\{(Y-\hat{f}(X))^2\}\\
&=E\{(f(X)+\varepsilon-\hat{f}(X))^2\}\\
&=E\{(f(X)-\hat{f}(X))^2\}+E\{\varepsilon^2\}+E\{(f(X)-\hat{f}(X))\varepsilon\}\\
&=E\{(f(X)-\hat{f}(X))^2\}+Var\{\varepsilon\}\tag{3}
\end{alignedat}$$
因为$\varepsilon - N(0,\sigma^2)$是随机噪声，服从正态分布，这个影响是无法避免的，是自然属性的。所以我们学习的过程中无法降低它。但是前面那项是可以避免的，是我们选取的函数{\hat(f)}的拟合误差，只要我们的学习方法足够好就可以降低。
# 2 机器学习的学习过程
我们在做机器学习时，是在给定来自整个样本空间的一个小训练数据集{data}，并在假定一个函数形式$\hat{f}$下来进行对$f$的学习。但是这里有两个问题，一是数据集$data$能否代表我们的整个样本空间，能将整个样本空间的特征表示完全吗，且在样本产生的时候由于随机噪声的存在，会引进新的模式(pattern)。第二个就是我们假设的模型函数$\hat{f}$是原始的系统关系$f$吗。这两者都会产生误差。  

由于第一个方面的原因，当我们的学习算法面对不同的训练数据集时，我们学习的最后的$f$是不同的，每个训练数据集的pattern大体上和整个样本空间是一致的,但是还是有差别，所以在给定一个训练数据集我们会学习一个$\hat{f_i}$。那么我们有多少个这样的$\hat{f_i}$，这取决于我们函数空间有多大。即我们的假设函数有多复杂有多柔性flexibility。越柔性我们的函数空间越大，我们的$\hat{f_i}$就越多，由于去训练数据集的不一致，当产生很多个$\hat{f_i}$，就会增大variance，因为多吗，不一致。  

由于第二方面的原因存在，会使我们的训练数据集包含其他的pattern，即我们的训练数据集不在仅仅有原来系统的pattern，还有噪声带来的pattern。那么在这种情况下，我们的学习算法会学习这些额外的pattern吗？  

下面偏差，方差分解的方式来对(3)式的第一项进行分解。

# 3 偏差方差分解
因为我们在学习的过程中，主要要降低的error就是(3)式中的第一项，现在将第一项进行分解
$$ \begin{alignedat} 
    \ error_1 &= E\{(f(X)-\hat{f}(X))^2\}\\
    &=E\{(f(X)+(-E\{\hat{f}(x)\}+E\{\hat{f}(x)\})-\hat{f}(X))^2\}\\
    &=E\{(f(X)-E\{\hat{f}(x)\})^2\}+E\{(E\{\hat{f}(x)\}-\hat{f}(X))^2\}\\ \ \ \ \ \ \ \ \ \ &+2E\{(f(X)-E\{\hat{f}(x)\})(E\{\hat{f}(x)\}-\hat{f}(X))\}\tag{4}
\end{alignedat}
$$
其中$E\{\hat{f}(x)\}$表示对函数空间的均值，即学习到的每一个函数$\hat{f_i}$做一个均值。所以(4)中的最后一项就为零，

$$
\begin{alignedat} 
\ error_1 &= E\{(f(X)-\hat{f}(X))^2\}\\
    &=E\{(f(X)-E\{\hat{f}(x)\}+E\{\hat{f}(x)\})-\hat{f}(X))^2\}\\
    &=E\{(f(X)-E\{\hat{f}(x)\})^2\}+E\{(E\{\hat{f}(x)\}-\hat{f}(X))^2\} \tag{5}
\end{alignedat} 
$$

式(5)中第一项表示我们所假设的函数空间$\hat{f}$的均值对真实的函数$f$对的偏差，第二项表示我们每给一个训练集的时候学习的函数$\hat{f_i}$的方差。记式为：
$$
\begin{alignedat} 
   \ error_1 &= bias(E\{\hat{f}(x)\})+variance(E\{\hat{f}(x)\})\\
    bias(E\{\hat{f}(x)\})&=E\{(f(X)-E\{\hat{f}(x)\}\\
    variance(E\{\hat{f}(x)\})&=E\{(E\{\hat{f}(x)\}-\hat{f}(X))^2\} \tag{6}
\end{alignedat} 
$$
我们给定一个函数空间，如果它的学习能力很强，将我们的噪声项的pattern学习误差都已经学习到了，但是这个pattern是随机，并不是系统的，所以用在其他的测试数据集上时就会增大误差，此时称为过拟合，overfitting。但是当我们的函数的假设空间很小时，即学习能力很弱时，我们就学习不完该数据集中原本属于系统的pattern，此时也会导致在测试数据集上的误差增大，这叫做欠拟合,underfitting。  

算法在学习时，总是倾向于将训练集上的误差减小的越小越好，所以当函数空间很大时，我们就要尽量搜索好的拟合函数，即将随机噪声产生的pattern也学习了。但是这会产生一个问题，即当我们的训练集改变时，此时随机噪声产生的pattern是不同，那么学习成果$\hat{f}$也是不一样的。那么我们给很多训练集时，学习到的$\hat{f}$就有很多，就会产生大的方差，虽然此时偏差是减小的。反过来当函数空间很小时，即我们的假设函数很简单，那么此时它的学习能力很弱，学习不同训练数据集带有的原来的pattern，那么就会使偏差增大，虽然此时方差是减少的。  

所以上面的偏差，方差总是相矛盾的，一个下降一个上升，有一个trade-off  
<center>

![bias-variance trade-off](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190321150047.png)
</center>

**综上**当函数空间很大时，任意给定的训练集，我们都能很好的拟合，即随机噪声的pattern都能拟合，但是此时选择的函数很多，每一次训练拟合的函数都很不一样，那么我们就会产生大的方差。当函数空间小时，我们的选择少，每一次拟合都不那么容易拟合，即排除了随机噪声产生的pattern。当我们的训练样本很大时，由大数定理，那么我们的训练集的pattern就和原来的pattern保持差不多一致，所以此时学习的函数$f$保持了很大一致。虽然此时偏差增大(随机项未拟合)，但我们的方差却减少了。

- 偏差：描述我们的模型的拟合的能力；
- 方差：描述我们的模型保持的一致性；
- 噪声：这是引起我们方差-偏差权衡的原因。

# 4 偏差方差的解决方案
模型越复杂，偏差减少，方差增大，模型越简单偏差增大，方差减少。而$error1=bias+variance$所以存在一个中间的复杂度使我们的$error1$最小。这就要权衡了。

在实际中我们往往是通过参数化的函数来对真实的函数进行近似，所以此时控制模型的复杂度，就有这些参数的数量、大小来决定。要控制模型的复杂度，我们就控制这些参数的数量，大小即可。所以，在训练时，将这些项加入到损失函数中。这就可以解释为什么岭回归，lasso回归，elastic回归里面会比正常的线性回归多了些项了。

当然还有个解释就是，由于我们对我们要拟合的模型有一个大致的了解，我们知道它其中一些参数的概率分布，即知道一些先验知识，那么此时在建模时我们就可以加入这些先验知识，用贝叶斯来解决问题。比如说岭回归就是输出的高斯分布加上模型参数的高斯分布用贝叶斯估计出来的。

往往我们在建立模型时，会考虑如何降低偏差，如何降低方差，但是这是一对矛盾的关系，只能看谁占优。降低偏差，比如boosting方法，降低方差，比如bagging方法,采用了bootstrap(自助采样)增加随机pattern的多样性，降低方差。但是训练数据集减少，增大了偏差。

reference
[1] Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani, An Introduction to Statistical Learning
[2] https://www.cnblogs.com/ooon/p/5711516.html
