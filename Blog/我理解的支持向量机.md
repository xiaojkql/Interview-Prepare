---
title: 我理解的支持向量机
categories:
- machine learning
tags:
- machine learning
- SVM
- classification
- regression
date: 2019-03-20 21:55:00
---

[KKT](https://www.cnblogs.com/liaohuiqiang/p/7805954.html)
[拉格朗日](https://www.jianshu.com/p/52aeaa540d25?utm_campaign)

# 1 一般的线性二分类问题
对于下图的线性二分类问题，做法是学习一个线性函数$f(x)=w^Tx+b$，然后再根据各个数据点与这个测试点的函数值的正负号来判断它的类别，判别公式为：
$$ y = sign(f(x)) = sign(w^T+b) \tag{1}$$
这个判别式很简单，但是它仅仅能做的就是判别，能给我们多大的确信度呢。而且有很多这样的判别器，哪一个最可靠呢？用什么来比较呢？  
所以自然而然的就产生一个判别标准：函数间隔  
**定义**对一个带标签的数据点，它对某一个函数的函数间隔为:
$$ \hat{\gamma}=yf(x)=y(w^Tx+b)\tag{2}$$  
所以此时我们就有一个度量的量来看看判别器的确信度。即$\hat{\gamma}$越大，我们判别的确信度越高。但是这又存在一个问题，对于一个超平面$f(x)=w^Tx+b$乘以一个系数a$af(x)=a(w^Tx+b)$还是那一个超平面,但是它的值确增大了a倍，此时对于同一个点，它的函数间隔不唯一。就自然而然的想新的解决方法。就是几何间隔。  
**定义**对于一个带标签的数据点，相对于一个函数的几何间隔为：
$$ \gamma = \frac{y(w^Tx+b)}{||w||}\tag{3} $$
即点到平面的距离，对几何间隔进行了规范化处理。每一个点到该分类器平面的距离是唯一的，所以可以用此来度量我们分类的确信度。

<center>

![](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190320192850.png)
</center>

# 2 支持向量机的直接推导(确信度，几何角度)
对于一个线性可分的数据集，存在多个分类的平面，那么那个分类平面值最好的平面，基于前面的介绍，总体确信度越大越好。数学上高告诉我们在下面三幅图中，第一幅的确信度最大，即其中间隔(margin)最大分类器最好。
间隔：一个**数据集**对一个平面的的最小的几何间隔。

<center>

![](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190320193537.png)
</center>

那么怎么求间隔呢？两条平行线之间的距离：
$$ margin=\frac{|b_1-b_3|}{||w||} \tag{4}$$
因为我们用正确分类，并在此时保证了最大间隔，所以对于正负点，应该满足于：
$$ w^Tx_{+}+b_2> b_1-b_2  \\
w^Tx_{-}+b_2< b_3-b_2 \tag{5}$$
因为一个超平面的$w$和$b$是可以按倍数进行缩小或者增大的，所以就取$b_1$和$b_3$之间满足关系$|b_1-b_3|=2$,而又有$b_2=\frac{(b_1+b_3)}{2}$，所以此时
$$ margin=\frac{2}{||w||}\\
w^Tx_{+}+b_2> 1\tag{6}\\
w^Tx_{-}+b_2< -1$$

我们要找确信度最大的，即最大化$margin$,而又要满足于一些约束，所以，学习问题就转化为一个优化问题
$$  max\ margin=\frac{2}{||w||}\\
s.t. \ w^Tx_{+}+b_2> 1\\
w^Tx_{-}+b_2<-1 \tag{7}$$
这就是支持向量机的原始形式了，改写为书上的形式
$$  min\ {\frac{1}{2}||w||^2}\\
s.t. \ y(w^Tx+b_2)> 1 \tag{8}$$
解释：即所有点的函数间隔要大于1，但是不仅要大于1，还要满足所有函数间隔中w最小的那个分类平面。所以即是在所有函数间隔大于1的情况下求最大的几何间隔。特殊情况(函数间隔为1)，当然也可以为其他任何值。  
因为系数$w$和$b$的可伸缩性，所以只要该直线正确分类了，那么就一定能找到函数间隔大于1的$w$和$b$。但是，数学告诉我们最大的几何只存在一个，所以能形成一个优化问题。
<center>

![](https://raw.githubusercontent.com/xiaojkql/Picture/master/img/deeplearning/RNN/20190320194155.png)
</center>

# 3 支持向量机的损失函数推导
用拉格朗日方法将上述的优化问题转化为一个无约束优化，即：
$$ min\ \frac{1}{2}||w||^2+C(1-y(w^Tx+b_2)) \tag{9}\\
C<0$$
因为$C<0$又是一个最小化问题，必然有$(1-y(w^Tx+b_2)<0$，所以转换成立。  

我们给一个函数间隔$\hat{\gamma}$，我们要求在此函数间隔下对一个训练数据进行分类，要求最准确，用0-1损失函数可以表达为：
$$ l =  \sum_{i=1}^{n}I(\hat{\gamma}-y_i(w^Tx_i+b)) \tag{10}$$
即所有点的函数间隔都必须满足于此，对于这样的$w$,$b$有很多个。怎么办？求几何间隔最大化，而几何间隔最大化由上面的推导得知可以在函数间隔为1时用，$min \ \frac{1}{2}||w||^2$表示。  
此时我们在求解时，需要做两件事，一是在给定函数间隔下，使分类误差率最低，即最小化
$$ l =  \sum_{i=1}^{n}I(\hat{\gamma}-y_i(w^Tx_i+b)) \tag{11}$$
第二件事即在该函数间隔正确分类的情况下，求几何间隔最大化，即最小化:
$$ min \ \frac{1}{2}||w||^2 \\
注：在函数间隔为1的情况下\tag{12}$$
都是最小化，两个过程我们可以用一个trade-off来表示：
$$ L= min \ (\frac{1}{2}||w||^2+C\sum_{i=1}^{n}I(\hat{\gamma}-y_i(w^Tx_i+b)))\tag{13}$$
解释：第一项在做几何间隔最大化，第二项在做给定函数间隔下分类误差率最小。那么怎样trade-off呢。  
**trade-off:**  
当C很大时，那么要最小化上面的东西，那么最小化的时候必然是先做后面的项，即先做函数间隔下误差率最小化。只有达到某一个阀值时，分类误差率很小时，才考虑做几何间隔最大化。几何间隔最大后，又做误差分类率最小化，再做几何间隔最大化，直到最后不能再做为止。  
当数据集完全线性可分时，我们可以设定C很大很大，那么一丁点的分类误差率就会导致损失函数很大，这样就不会考虑做几何间隔最大化，那么就要将分类误差率降低为0后再来做几何间隔最大化。所以此时就是前面的硬间隔最大化。  
当数据集不完全线性可分时，可以降低C的值，即还没有将分类误差率降低，还没有完全分类正确时，我们就开始做几何间隔最大化，由于trade-off那么我们就不必要求最后是完全的分类正确，此时就是柔性间隔最大化。  
综上，支持向量机是在优化一个0-1损失加上一个正则化项的损失函数。0-1损失对应的做分类正确，正则化做几何间隔最大化。

# 4 hinge损失
由于上面的式子中第二项0-1损失函数不连续，有一个跳点，不易于优化。所以我们要寻找一个代理函数来代替0-1损失函数，此时就是用hinge loss来处理
$$ hinge(z) =max(0,1-z) \tag{14}$$
因为损失函数转换为
$$ L= min \ (\frac{1}{2}||w||^2+C\sum_{i=1}^{n}max(0,\hat{\gamma}-y_i(w^Tx_i+b)))\tag{15} $$
即
$$ L= min \ (\frac{1}{2}||w||^2+C\sum_{i=1}^{n}hinge(\hat{\gamma}-y_i(w^Tx_i+b)))\tag{16} $$
因为hinge损失里面有一个$1-z$完美契合了SVM的损失函数中$I(\hat{\gamma}-y_i(w^Tx_i+b)$，而hinge又与0-1损失函数保持一致性。  
所以SVM就是在优化hinge损失下加上一个正则化项，其中Hinge损失对应降低分类误差率，而正则化项在做几何间隔最大化。提高确信度。  
下面对(15)式进行改写  
因为$max(0,\hat{\gamma}-y_i(w^Tx_i+b)\geq0$，$\xi=\hat{\gamma}-y_i(w^Tx_i+b)$，则$\xi_i\geq0$。
