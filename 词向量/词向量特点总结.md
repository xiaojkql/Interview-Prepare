### 1 词向量
独热的表示不具蕴涵词的任何语义信息，仅仅是将词进行了符号化，那么如何将词义赋予给词的表示呢？分布式表示，用词的上下文表示词，即词的表示要体现出词的上下文环境。
那么就出现了基于分布假说的词表示方法，主要体现在一是上下文表示 二是词与其上下文关系之间的建模不同
- 基于矩阵的分布式表示：两个词的词义相似度可以转换为两词的欧式距离
  矩阵有：词词共现矩阵，词文档共现矩阵，词n-gram共现矩阵
- 基于聚类的分布式表示
- 基于神经网络的分布式表示

词向量的优势，可以用向量的数学运算就可以捕捉语义与语法的特性

#### 1.1 概述
学习过程，用一个词向量表示一个词，而该向量能够体现该词的语义，而该词的语义又是其所在的上下文来体现，所以在学习该词的向量时就是在学习该词的上下文环境。那么怎样学习该词的上下文环境呢？word2vec就是基于预测模式，如果某一个词出现在了该词的上下文那么基于该词去预测它的上下文的词应该包含该词。glove是基于全局的共现矩阵，用两词词向量的内积来模拟两词之间共现程度。

#### 1.2 词向量评估
1 内部评估 (内涵评估)
- 相关性（relatedness）：对于一对相关度比较高的词，它们的词向量的余弦相似度也很高吗？这里作为标准的相关度评分由人工给出。其中，文章提出了一种新的方法，称为一致性检查：给出一组单词，看词向量能否找出类别不同的那一个
- 类比性（analogy）：对给定的词y，能否找到一个对应的词x，使x与y的关系能够类比另两个已知词a与b之间的关系。例如比较经典的king-queen与man-woman之间的关系
- 类别化（categorization）：对词向量进行聚类，看每个簇与有标记数据集各类相比纯度如何
- 选择倾向（selectional preference）：判断某些名词是更倾向做某个动词的主语还是宾语，例如一般顺序是people eat而不是eat people
  相似与类别任务的不同，前者用向量之间的距离或者夹角来进行测量，标量，而后者用词向量的各个维度来进行度量，捕捉了分布式
2 定量评价
2.1 具体应用评估
2.2 单纯词向量
- 语义相似度评估
- 类别评估
- 
  词义上，诸如 马云-阿里巴巴+腾讯≈马化腾，刘邦-汉朝+秦朝≈嬴政
  词法上，
  推断不同单词之间的语义关系的能力
3 定性评价
加已经学习的词向量进行降维，绘图进行评估
词向量一般是后续NLP模型的特征，后续NLP模型效果好，词向量才算好，没有任何理由表明词类比性质有助于后续NLP模型更好
语义相同意味着上下文分布一样，而上下文的分布一样就意味着它们跟哪个词的互信息都一样（它们交往的朋友是谁、交往的密切程度如何都一样）

### 2 word2vec
####  特点
- Word2Vec比如SVD之类的更容易训练更多的数据
- 基于word和context做文章，即可以理解为模型在学习word和context的co-occurrence
- 依赖于前后的词，而不仅仅是依赖于前面的词
- 去掉了一个隐藏层，降低了学习速率
- 噪声分布 Q(w) 只是用来采样，而不参与计算
- 负采样分布有一个幂，作用是打破修护一丁点的不平衡，即将高频的降低，将低频的提高
- 上下文相似的词，词向量也会接近,对于词向量模型来说，词相近就意味着它们具有相似的上下文分布
- 当语料足够大时，简单的模型也能得到较好的词表示
- simple is powerful
- 二次采样技术 对于高频词，通过一定的概率对该词进行抛弃
- 用词向量来计算概率

####
词的频率，多个词的共现率，相关的矩阵

### 3 Glove 特点(tricks)
给定一个词，另外两个词的相似性，可以用它们与给定词的共现率来表示，
- 共现矩阵(co-occurrence matrix) -- 语料库的统计信息，某个词的共现的词与另一个词共现的词几乎相同的话，那么称此两词具有相似性。归纳了统计信息
- 上下文窗口(context window)
- 加权函数，对低频词进行衰减，同时高频词不变，
- 结合了全局矩阵分解和局部语言环境
- 在全局的统计信息上学习，而非是在一个局部的环境中进行学习
- 相比于全局矩阵分解，glove向量的优化程度更高
- 相比于word2vec
  |Glove|Word2Vec|
  |回归|分类|
  |全局统计信息|局部统计信息|
  |矩阵|神经网络语言模型|

### 4 FastText
- 引入形态学特征，对大词库以及奇特的词比较多的情况很好，词的构成
- a bag of character n-gram， 对于每个词汇的向量都有一些共享的参数
- 计算速度快，能够在大语料库上使用
- 能够用已有的字符n-grams计算OOV的词向量
- 每个词的词向量由组成其的character n-grams的向量的和
- 两个相关的工作：词的形态学向量，字符级的特征
- 用字符级的特征考虑词的形态学
- 改变了scoring function
- 添加<,>以区分前缀和后缀，同其他字符序列进行区别
- 最终每个词的词向量由它的n-grams(3<=n<=6)和它本身的向量和构成
- 节省内存，用到了hashing tricks


### 5 ELMo
好的词向量：a.蕴涵丰富的词的句法与语义信息；b.能够根据不同的语言环境而作出变化
- 用到了词的多个维度的特征，包括句法与语义特征
- 可以模拟多义词
- 词向量是双向语言模型的内部状态的函数
- 使用网络内部的参数是重要的
- 词向量是整个句子的函数
- 是对输入层上所有隐藏层的输出的一个线性组合(提高性能)
- 高层的输入表示不同的语义信息(可用于语义区分任务)，底层表示句法信息(可用于词性标注)
- 对高层和底层进行组合可以让下游任务自由选择词的信息，(作为半监督学习的一部分)----> 多层网络的不同层encoding不同类型的信息
模型细节
- 通过对字符级的序列进行卷积操作获取形态特征
- 只使用了单语言，可以获得大量训练数据
- 修改了损失函数，损失函数是两个方向上的对数似然函数的和
- 两层BiLSTM，每一个方向的参数不共享
- 半监督学习，首先在大量的无标记的数据集上进行预训练，然后将之融合到下层的NLP任务，要求较少的特定任务的数据集
- 针对特定的任务时，使用了一个权重因子(softmax形式)对所有层的输出进行组合。然后加一个缩放因子，此缩放因子对优化的过程非常重要
- 使用了layer normalization在每一层weighting之前
- 使用了dropout
- residual connection
- 使用了L-2regularizion,可以使每一层的参数接近所有层 参数的平均值
- 联合任务时，首先freeze elmorep 然后与其本身的x进行concate
- 联合任务的输出层也可以将elmo_task与最后的输出层进行concate
- 在某些任务上进行fine-tuning时会降低perplexities，但是会提高下有任务的质量

