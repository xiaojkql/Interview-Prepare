两阶段模式:预训练阶段+finetuning阶段
非监督表示学习
- 以自回归语言模型为优化目标或者以自编码语言模型为优化目标


自回归语言模型：ELMO,GPT,LSTM都是自回归语言模型
- 优化目标：前向的条件概率或者后向的条件概率之积
- 神经网络起到模拟其中的条件概率得作用
- 更加接近真实的语言产生过程
- shallow bidirectional

自编码语言模型：
- BERT Denoising Autoencoder(降噪自编码器 DAE  引入噪声)
- 实现了双向语言(deep bidirectional)
- 忽略了mask与mask之间的依赖关系，造成依赖缺失，即每个mask的预测都是独立的，而不是像AR那样计算一个联合概率，而且是被直接替换的是看不到其本身的信息，所以会造成长依赖关系的损失
- 没有做明显的概率密度估计，而是做了从被污染的数据里面进行恢复成原始数据
- pretrain与finetune之间的差异mask


## 2 张俊林系列博客
### 2.1 从Word Embedding到Bert模型—自然语言处理中的预训练技术发展史
自然语言的训练过程
feature-based fine-tuning
pre-trained fine-tuning
#### 图像领域的预训练
Frozen或者finetuning
预训练优点：a.弥补训练数据的不足;b.加快模型的收敛;c.更好的初始点
为什么可行？ 底层特征都是一些可复用的特征(线段,图形框),高层特征才是与任务相关的特征.
#### word embedding
- 最基础: 语言模型 预训练的目标  (语言模型: 一句话出现的概率,具体求时将求整句话的概率转换成了基于前面的词汇预测后面词汇的概率)
- 原始: NNLM (最古老的神经网络语言模型) 功能:a.根据上文预测下文(主要,解决语言模型的一个网络结构,一定要上文预测下文,对于预测的时候就是这样来做的,不能随心所欲的改变训练流程);b.产生一个副产品词向量.
- 闪亮登场: word2vec CBOW(窗口小,有上下文)和skip-gram两种训练方式;功能:获取词向量-最主要的功能,而基础是语言模型,在类似语言模型的基础上改变训练的方式.加快了获取词向量的速度
- 登场: 使用word2vec, 可以将word2vec看成one-hot层到embedding层网络参数的初始化. 两种方式: a.frozen;b.fine-tuning.
- 缺点: 不能解决词的多义,静态的,所以NLP下游任务的效果并不是被提的很高

#### ELMO
原来的word embedding是学到了就把它取出来,看成是每个单词的一个固定的词向量代表
ELMO不再将底层的One-hot到embedding层的权重矩阵取出来赋予给每个单词,而是在其上加上一层网络学习它的上下文信息
ELMO: 底层表示一个词的主要语义信息(一个词的主要意思), 上层混入了上下文信息(所以可以混入该词所处的环境,句法信息,语法信息,根据上下文而来的词义信息)
三层网络: 单词特征+句法特征+语义特征
使用:用加权的方式将三层的向量进行加权求和
缺点:使用的双向LSTM,训练速度慢,特征抽取能力弱于Transformer(拼接方式的硬核式双向弱于transfomer)

word embedding 和 ELMO都需要外加一些任务相关的网络结构,而不是像图像领域的预训练模式

#### GPT
现代NLP预训练的开山鼻祖
新的NLP预训练模式,一个大网络,改造下游任务的输入形式形成GPT的输入形式
缺点：是单向的

#### Bert
(4大任务:序列标注,序列分类,序列关系,序列生成) bert原始论文没有做序列生成类任务
集大成者
- 使用了GPT的预训练模式
- 主要创新点:使用了双向,即引进了新的任务,即将双向引入到了训练过程中,使用了上下文信息
- 双向语言模型起到绝对核心的作用
- 预测下一个句子起到了一定的影响力作用

#### word embedding和ELMO和GPT和Bert的关系
Bert,ELMO是双向的
BERT和GPT都使用了Transfomer作为特征提取器(更深,更好)
BERT借鉴了Word embedding中的CBOW的语言模型上下文信息

#### 总结
ELMO,GPT,BERT开创了将语言学先验知识引入任务的方法



### 3 效果惊人的GPT 2.0模型：它告诉了我们什么

