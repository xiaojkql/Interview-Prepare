两阶段模式:预训练阶段+finetuning阶段
非监督表示学习
- 以自回归语言模型为优化目标或者以自编码语言模型为优化目标


自回归语言模型：ELMO,GPT,LSTM都是自回归语言模型
- 优化目标：前向的条件概率或者后向的条件概率之积
- 神经网络起到模拟其中的条件概率得作用
- 更加接近真实的语言产生过程
- shallow bidirectional

自编码语言模型：
- BERT Denoising Autoencoder(降噪自编码器 DAE  引入噪声)
- 实现了双向语言(deep bidirectional)
- 忽略了mask与mask之间的依赖关系，造成依赖缺失，即每个mask的预测都是独立的，而不是像AR那样计算一个联合概率，而且是被直接替换的是看不到其本身的信息，所以会造成长依赖关系的损失
- 没有做明显的概率密度估计，而是做了从被污染的数据里面进行恢复成原始数据
- pretrain与finetune之间的差异mask
