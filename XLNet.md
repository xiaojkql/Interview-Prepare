XLNet: 具体怎么做才能让语言模型：看上去仍然是从左向右的输入和预测模式，但是其实内部已经引入了当前单词的下文信息呢？XLNet在模型方面的主要贡献其实是在这里。

XLNet集成了BERT的深度的双向语言模型,GPT的自回归模型,TransformerXL的
双向的不够,但是双向了,自回归的不标准,但是自回归了,集合两者的优势取中间的优势

# 0 知乎相关
[如何评价在20个任务上超越BERT的XLNet？](https://www.zhihu.com/question/330307904)
- 模型(model),task,数据三大关键
- model structure  存储知识的结构
- task design (训练model的方法) 目标能间接的学到下游任务相关的知识
- 自监督学习任务
- fine-tuning 抛弃与下游任务无关的知识,但是进一步强化与下游任务相关的知识
- 在模型和数据固定下,好的训练任务或者(多个任务的组合)的改进可以较大幅度的提升效果; 改变model structure或许提升的效果并不怎么明显

# 1. 模型对比,与BERT相比有什么异同？有什么改进？对于什么场景应用有效？为什么呢？
优势: 在生成类任务中应该有很大的优势
使用了更好的数据
引进了PLM使预测与训练时的输入数据保持一致性,自然语言理解和自然语言生成一致了 
使用了TransfomerXL结构,对长文本数据更加友好

## 1.2 XLNet起作用的改动项
- PLM使得预训练和使用/微调过程的输入保持了一致性.
- 引入了TransformerXL里面的一些思想: 相对位置编码和分段RNN机制(对长文档任务的帮助): 对于长文档,特别是一些阅读理解任务提升的很多.
- 使用了更大,质量更高的数据集.

PLM和TransformerXL都能提升效果,但是TransformerXL是对长文档任务效果的提升

# 2. 两种不同的语言模型
自回归语言模型：ELMO,GPT,LSTM都是自回归语言模型
- 优化目标：前向的条件概率或者后向的条件概率之积
- 神经网络起到模拟其中的条件概率得作用
- 更加接近真实的语言产生过程,与NLP下游任务中的生成式任务完全匹配
- shallow bidirectional

自编码语言模型：
- BERT Denoising Autoencoder(降噪自编码器 DAE  引入噪声)
- 实现了双向语言(deep bidirectional)
- 忽略了mask与mask之间的依赖关系，造成依赖缺失，即每个mask的预测都是独立的，而不是像AR那样计算一个联合概率，而且是被直接替换的是看不到其本身的信息，所以会造成长依赖关系的损失
- 没有做明显的概率密度估计，而是做了从被污染的数据里面进行恢复成原始数据
- pretrain与finetune之间的差异mask

XLNet: 融合了两者的优点进行了互补

# 3 Permutation language model
基本思想: x1,x2,x3,x4. x3作为预测目标上文信息为x1,x2;下文信息是x4. 如果从左到右则输入是x1,x2没有下文信息x4. 如果我们改变词序x4,x2,x3,x1那么就可以再从左向右的进行输入,那么此时就可以用到下文信息x4.

总体的输入仍然是x1,x2,x3,x4的形式. 那么怎么实现上面的思想,就只能在transformer的结构计算上做文章了. 具体怎么做就是在attention机制的实现上加入掩码机制来实现.

双流注意力机制模型来实现: 内容流注意力机制; Query流注意力机制.

BERT为什么引入mask: x1,x2,x3, 预测x3将x3标记为mask如果不标记为mask经过多层的自注意力机制就看到了自己就不是预测了.
而XLNET通过一个query流来实现mask的功能.实现的方式不一样

